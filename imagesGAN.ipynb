{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfQkNwKSf7XI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, Flatten, Dropout, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.layers import MaxPooling2D\n",
        "\n",
        "from matplotlib import pyplot\n",
        "import numpy as np \n",
        "from numpy import expand_dims, ones, zeros\n",
        "from numpy.random import rand, randint, randn\n",
        "\n",
        "from keras.layers import Conv2DTranspose, Reshape\n",
        "from keras.models import load_model \n",
        "from random import choice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBcfqJ8knfA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6E3IWQpvlgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path='/content/drive/My Drive/train/'\n",
        "save_path = '/content/drive/My Drive/generated/'\n",
        "image_size = 256\n",
        "opt = Adam(lr=0.0002, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AO4s7KloV1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_image(path='/content/drive/My Drive/train/', \n",
        "                 output_shape=None):\n",
        "    imageDG = ImageDataGenerator()\n",
        "    images_iterator = imageDG.flow_from_directory(directory=path,\n",
        "                            target_size=output_shape,\n",
        "                            class_mode='categorical',\n",
        "                            batch_size=128,\n",
        "                            interpolation='nearest')\n",
        "    x, y = next(images_iterator)\n",
        "    x = (x - 127.5) / 127.5\n",
        "    return x\n",
        "\n",
        "# select real samples\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "\t# choose random instances\n",
        "\tix = randint(0, dataset.shape[0], n_samples)\n",
        "\t# retrieve selected images\n",
        "\tX = dataset[ix]\n",
        "\t# generate 'real' class labels (1)\n",
        "\ty = ones((n_samples, 1))\n",
        "\treturn X, y\n",
        "\n",
        "# generate n fake samples with class labels\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "\t# generate uniform random numbers in [0,1]\n",
        "    x_input = generate_latent_points(latent_dim, n_samples)\n",
        "    X = g_model.predict(x_input)\n",
        "    y = zeros((n_samples, 1))\n",
        "    return X, y\n",
        "\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    x_input = randn(latent_dim*n_samples)\n",
        "    x_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return x_input \n",
        "\n",
        "def show_images(dataset):\n",
        "    length = len(dataset)\n",
        "    index = randint(0, length, 16)\n",
        "    dataset = dataset[index]\n",
        "    for i in range(16):\n",
        "        pyplot.subplot(4, 4, 1+i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(dataset[i])\n",
        "    pyplot.show()\n",
        "def save_plot(examples, epoch, n=4):\n",
        "    examples = (examples + 1)/2.\n",
        "    for i in range(n*n):\n",
        "        pyplot.subplot(n, n, 1+i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(examples[i])\n",
        "    file_name = 'generated_plot_e%03d.png' % (epoch+1)\n",
        "    pyplot.savefig(save_path+file_name)\n",
        "    pyplot.close()\n",
        "\t\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n",
        "\t# prepare real samples\n",
        "\tX_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "\t# evaluate discriminator on real examples\n",
        "\t_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
        "\t# prepare fake examples\n",
        "\tx_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "\t# evaluate discriminator on fake examples\n",
        "\t_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\t# summarize discriminator performance\n",
        "\tprint('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
        "\t# save plot\n",
        "\tsave_plot(x_fake, epoch)\n",
        "\t# save the generator model tile file\n",
        "\tfilename = 'generator_model_%03d.h5' % (epoch+1)\n",
        "\tg_model.save(save_path+filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb_B4betf2Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = process_image(output_shape=(image_size,image_size))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjYXUzYyPkx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_images(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drr1ATGeE2do",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the standalone discriminator model\n",
        "def define_discriminator(in_shape=None):\n",
        "    model = Sequential()\n",
        "    # normal\n",
        "    model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    # downsample\n",
        "    model.add(Conv2D(128, (3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))  \n",
        "\n",
        "    model.add(Conv2D(256, (3,3), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))  \n",
        "    \n",
        "    # classifier\n",
        "    model.add(Flatten())\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile model\n",
        "    # opt lr=0.0002, beta_1=0.5\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def define_generator(latent_dim):\n",
        "    model = Sequential()\n",
        "    # foundation for 4x4 image\n",
        "\n",
        "    n_nodes = 256*4*4\n",
        "    model.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    model.add(Reshape((4, 4, 256)))\n",
        "    # 8x8\n",
        "    model.add(Conv2DTranspose(64, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    # 16x16\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))  \n",
        "    # 32x32\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    # 64x64\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    # 128x128\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "    # 256x256\n",
        "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "    model.add(LeakyReLU(0.2))\n",
        "\n",
        "    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "    return model\n",
        "\n",
        "def define_gan(g_model, d_model):\n",
        "    d_model.trainable = False\n",
        "    model = Sequential()\n",
        "    model.add(g_model)\n",
        "    model.add(d_model)\n",
        "\n",
        "    # lr=0.0002, beta_1=0.5\n",
        "    # opt = Adam(lr=0.0003, beta_1=0.5)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViGUNUBQaIsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=3000, n_batch=32):\n",
        "    bat_per_epo = int(dataset.shape[0]/n_batch)\n",
        "    half_batch = int(n_batch/2)\n",
        "    for i in range(n_epochs):\n",
        "        # for j in range(bat_per_epo):\n",
        "        d_loss = 10\n",
        "        count = 0\n",
        "        while d_loss > 0.05:\n",
        "            if count>20:\n",
        "                break\n",
        "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "            # d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
        "                \n",
        "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "            x_t = np.concatenate((X_real, X_fake))\n",
        "            y_t = np.concatenate((y_real, y_fake))\n",
        "\n",
        "            # d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
        "            d_loss, _ = d_model.train_on_batch(x_t, y_t)\n",
        "            # print('>%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n",
        "            print(\">{} {} training discriminator: d_loss={}\".format(i, count, d_loss))\n",
        "            count += 1\n",
        "        \n",
        "        g_loss = 10.\n",
        "        count = 0\n",
        "        while g_loss > 0.1:\n",
        "            if count>10:\n",
        "                break\n",
        "            X_gan = generate_latent_points(latent_dim, n_batch)\n",
        "            y_gan = ones((n_batch, 1))\n",
        "\n",
        "                # y_gan = y_gan \n",
        "\n",
        "            g_loss, _ = gan_model.train_on_batch(X_gan, y_gan)\n",
        "            print(\">{} {} training generator: g_loss={}\".format(i, count, g_loss))\n",
        "            count += 1\n",
        "            \n",
        "        # print('>%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f' %(i+1, j+1, bat_per_epo, d_loss, g_loss))\n",
        "        #if (i+i) % 10 == 0:\n",
        "        summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "        images, _ = generate_fake_samples(g_model, latent_dim, n_batch)\n",
        "        images = (images+1)/2.0\n",
        "        show_images(images)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9LHOR7YfIBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = images\n",
        "# process_image(output_shape=(image_size, image_size))\n",
        "latent_dim = 100\n",
        "d_model = define_discriminator(in_shape=(image_size,image_size,3))\n",
        "g_model = define_generator(latent_dim)\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# gan_model.summary()\n",
        "# plot_model(gan_model, to_file=path+'gan_plot.png', show_shapes=True, show_layer_names=True)\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttIVPqkIlk1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "g = load_model(save_path+'generator_model_2920.h5')\n",
        "inp = generate_latent_points(100, 1)\n",
        "new_im = g.predict(inp)\n",
        "pyplot.imshow(new_im[0])\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE_gLDtSMaA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}