Problems: 

Optimizer is wrong,  The discriminator cannot improve.
The discriminator is so bad that the generator cannot be updated because of low loss.
It's hard to find a convergence status. 
dataset is not enough
G_loss cannot be reduced.
Memory is not enough

7.  activation problems, can not find a activation to restrict the range of the generator output. So we want to invent a new activation function. 






attempts:
Dynamic training instead of fixed number of discriminator training or fixed number of generator training. ( use while loop to check the loss, then decide whether break the corresponding loop.)
Set loss as the detector. if loss is less than a number(0.01, which has relation to the batch size.), stop. begin to train the other one. 

2D:
Add flatten layer Generator model. 
Changed the discriminator.trainable of GAN to True. make the discriminator to update when train generator.
Add maxpooling2d to discriminator.
change the batch size
Drop useless data.
Add more columns for real data.
Different encoder : one hot encoding, label encoding, label binary encoding. 



10. Invent a new activation function for the last layer.











Future plan:

Conditional GAN.
Now we only have 2 classes: fake and real. 
	In the future, We want to try more classes. 

	1. Divide the real data into 2 classes according to the hla_type_class. 
		y_real_class1 = 1
		y_real_class2 = 2


	2. Divide the random data into 2 classes: 

		x_random_class1 = range: 0-0.5, 
		x_random_class2 = range: 0.5-1, 
		y_fake_class1 label = 1,
		y_fake_class2 label = 2.
		fake_label = 0

		If we want to generate sequences of class 1, we use the random range(0-0.5) as 		input of GAN. Otherwise, we use range(0.5-1) as input of GAN.

2. Try fixed input instead of random numbers. 

2. Add embedding to the model

3. Try 3D protein dataset

4. Add validation data when train discriminator.




	
	
